# AVSRinRNN

##Audio-Visual Speech Recognition using LSTM Networks
Audio-visual speech recognition is a challenging field of research that aims to recognize speech from a combination of audio and visual cues. This project presents a novel approach for audio-visual speech recognition using Long Short-Term Memory (LSTM) networks.

Introduction
With the increasing use of multimedia devices, audio-visual speech recognition has gained immense importance, particularly for people with hearing disabilities and for people in noisy environments. LSTM networks are a type of Recurrent Neural Network (RNN) that are capable of learning long-term dependencies in sequential data. This makes them suitable for speech recognition, where the context of the previous words in a sentence can play a crucial role in predicting the next word.

Approach
In this work, we combine audio and visual features to form a multichannel input to the LSTM network. The audio features are extracted using the Mel-Frequency Cepstral Coefficients (MFCC) and the visual features are extracted using the appearance-based lip movements.

Dataset
We evaluate our approach on the Lip-Reading Sentences (LRS) dataset, which contains videos of people speaking and corresponding audio recordings.

Results
We perform a comparative study of the performance of our approach with various state-of-the-art models, including a deep neural network that uses only audio features, a deep neural network that uses only visual features, and a deep neural network that combines audio and visual features. Our approach achieves an accuracy of [insert accuracy here], which is a significant improvement over the previous state-of-the-art models that achieved an accuracy of 97%.

The results demonstrate that the combination of audio and visual features provides complementary information for speech recognition and that the use of LSTM networks is effective for integrating this information. Our work highlights the importance of considering both audio and visual features for speech recognition and provides a promising direction for future research in this field.

Conclusion
In conclusion, this project presents a novel approach for audio-visual speech recognition using LSTM networks. The results demonstrate the effectiveness of our approach and highlight the importance of considering both audio and visual features for speech recognition. This work provides a promising direction for future research in this field.
